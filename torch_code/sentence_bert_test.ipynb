{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import argparse\n",
    "from utils.utils import load_config\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from utils.utils import ckpt_save\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SentenceTransformerTrainingArguments, losses, SentenceTransformerTrainer\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, SimilarityFunction\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# seed Í≥†Ï†ï\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred): \n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # logitsÏùÑ Ï†úÎåÄÎ°ú Ï∞®Ïõê Ï∂ïÏÜåÌïòÎäîÏßÄ ÌôïÏù∏\n",
    "    predictions = logits.squeeze()\n",
    "\n",
    "    # labelsÍ∞Ä ÌÖêÏÑúÎ°ú Îì§Ïñ¥Ïò¨ Í≤ΩÏö∞ numpyÎ°ú Î≥ÄÌôò\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    # Pearson ÏÉÅÍ¥ÄÍ≥ÑÏàò Í≥ÑÏÇ∞\n",
    "    pearson_corr, _ = pearsonr(predictions, labels)\n",
    "\n",
    "    return {\"pearson_corr\": pearson_corr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_path = \"../../data/train.csv\"\n",
    "train_dataset = pd.read_csv(train_path)\n",
    "train_dataset = train_dataset[[\"sentence_1\",\"sentence_2\",\"label\"]]\n",
    "train_dataset[\"label\"] = scaler.fit_transform(np.array(train_dataset[\"label\"]).reshape(-1,1))\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "\n",
    "val_path = \"../../data/dev.csv\"\n",
    "val_dataset = pd.read_csv(val_path)\n",
    "val_dataset = val_dataset[[\"sentence_1\",\"sentence_2\",\"label\"]]\n",
    "val_dataset[\"label\"] = scaler.transform(np.array(val_dataset[\"label\"]).reshape(-1,1))\n",
    "val_dataset = Dataset.from_pandas(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='730' max='730' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [730/730 05:55, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.027847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "train_loss = losses.MSELoss(model=model)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"./saved_model/KR_SBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    loss=train_loss,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=val_dataset[\"sentence_1\"],\n",
    "    sentences2=val_dataset[\"sentence_2\"],\n",
    "    scores=val_dataset[\"label\"],\n",
    "    main_similarity=SimilarityFunction.COSINE,\n",
    "    name=\"sts-dev\",\n",
    ")\n",
    "dev_evaluator(model)\n",
    "\n",
    "model.save_pretrained(\"./saved_model/KR_SBERT_TEST/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0300, device='cuda:0')\n",
      "tensor(-0.0651, device='cuda:0')\n",
      "tensor(-0.0302, device='cuda:0')\n",
      "tensor(-0.0216, device='cuda:0')\n",
      "tensor(-0.0462, device='cuda:0')\n",
      "tensor(-0.0530, device='cuda:0')\n",
      "tensor(-0.0036, device='cuda:0')\n",
      "tensor(-0.0465, device='cuda:0')\n",
      "tensor(-0.1353, device='cuda:0')\n",
      "tensor(-0.0226, device='cuda:0')\n",
      "Complete Extract ouptut.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model load and test\n",
    "\n",
    "test_path = \"../../data/test.csv\"\n",
    "test_dataset = pd.read_csv(test_path)\n",
    "test_dataset = test_dataset[[\"sentence_1\",\"sentence_2\"]]\n",
    "\n",
    "\n",
    "test_model = SentenceTransformer(\"./saved_model/KR_SBERT_TEST/final\")\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(test_dataset)):\n",
    "    sentence_1 = test_dataset.iloc[i][\"sentence_1\"]\n",
    "    sentence_2 = test_dataset.iloc[i][\"sentence_2\"]\n",
    "    \n",
    "    embeddings = test_model.encode([sentence_1, sentence_2], convert_to_tensor=True)\n",
    "    \n",
    "    data['similarity'] = [util.cos_sim(sent1, sent2).squeeze() for sent1, sent2 in tqdm(zip(vec1, vec2), total=len(data))]\n",
    "    # score = 2.5 * (similarities.item() + 1)\n",
    "    # print(f\"Similarities = {similarities.item()}\")\n",
    "    if similarities < 0 :\n",
    "        print(similarities)\n",
    "    # print(f\"Score = {score}\")\n",
    "    predictions.append(similarities)\n",
    "\n",
    "predictions = list(round(float(i), 1) for i in predictions)\n",
    "output = pd.read_csv(\"../../data/sample_submission.csv\")\n",
    "output[\"target\"] = predictions\n",
    "output.to_csv('./output/output_SBERT.csv', index=False)\n",
    "print(\"Complete Extract ouptut.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
