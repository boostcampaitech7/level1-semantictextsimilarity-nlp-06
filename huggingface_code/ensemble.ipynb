{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적의 조합\n",
    "**`제출 기준`**  \n",
    "- kf-deberta, roberta-large, kr-electra, electra-kor  \n",
    "  - validation pearson: 0.9371  \n",
    "  - private: 0.9276  \n",
    "  - public: 0.9342  \n",
    "\n",
    "**`아쉽게 제출하지 못한 것`**  \n",
    "- kf-deberta, kr-electra, ko-electra  \n",
    "  - validation pearson: 0.9390  \n",
    "- kf-deberta, roberta-large, kr-electra, ko-electra  \n",
    "  - validation pearson: 0.9396  \n",
    "\n",
    "### 앙상블 사용법\n",
    "클래스는 `ensemble.py`에서 확인  \n",
    "최종 선정된 모델은 [Google Drive](https://drive.google.com/drive/folders/1BbR8wUIGyn3wSNl3zUOZ-GwbGzURYqrp?usp=sharing)에서 다운 받기  \n",
    "\n",
    "**`config_paths`**  \n",
    "fine-tuning 시 사용했던 config.yaml의 경로를 string list로 입력  \n",
    "이때 모델과 config.yaml은 해당 위치의 `models`라는 폴더 안에 넣어두기(구글 드라이브에서 받은 폴더를 해당 dir에 바로 덮어씌워도 됨)  \n",
    "config.yaml의 model_name에서 배포자명을 제외한 모델의 이름과 저장한 pt 파일의 이름이 일치하도록 하기(문제가 된다면 수정 예정)  \n",
    "- 예:  \n",
    "  ```yaml\n",
    "  model_name: klue/roberta-small\n",
    "  ...\n",
    "  ```\n",
    "  위와 같이 지정된 경우, 저장된 모델의 이름은 `roberta-small.pt`\n",
    "\n",
    "**`base_predictions()`**  \n",
    "fine-tuning을 수행한 LLM들을 사용해서 base predictions를 추출  \n",
    "데이터 개수만큼의 행, 모델 수만큼의 열이 생성  \n",
    "\n",
    "**`stacking`, `kfold_stacking`, `soft_voting`**  \n",
    "각각 기본 stacking, soft voting, kfold stacking으로 메타 모델을 학습  \n",
    "clf 인자에 'linear'를 입력하면 LinearRegression을, 'lgbm'을 입력하면 LightBGM 모델을 사용하고 클래스의 멤버 변수로 등록  \n",
    "clf를 바꾸고 싶다면 clf 인자를 변경해서 함수를 다시 호출하면 됨  \n",
    "kfold stacking은 원래 kfold를 적용한 데이터로 LLM부터 학습해야 하지만 시간 관계 상, 리소스 관계 상 메타 모델에만 k-fold를 적용하는 방식으로 구현  \n",
    "n 값으로 몇 개의 fold를 사용할지 지정  \n",
    "\n",
    "**`inference`**  \n",
    "sample_submission.csv를 읽어와 앙상블 결과 저장  \n",
    "is_voting이 True이면 soft voting 결과를 사용하고, False이면 stacking과 kfold_stacking 중 가장 마지막에 사용한 classifier를 기준으로 추론 수행  \n",
    "\n",
    "**`simulate`**  \n",
    "임의로 두 개의 문장을 생성하여 추론을 수행하면 라벨 값이 출력  \n",
    "마찬가지로 is_voting이 True이면 voting의 결과를 사용하고, False이면 stacking과 kfold stacking 중 가장 마지막에 사용한 classifier를 기준으로 수행  \n",
    "앙상블에 사용한 모델의 base prediction과 최종 앙상블 결과가 함께 반환  \n",
    "이 때 base prediction은 config_paths에서 지정한 순서와 같은 순서로 저장됨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta base 제외 - 쩔수 없었음;;\n",
    "config_paths_1 = ['./models/roberta_large.yaml',\n",
    "                  './models/kf_deberta_cross_sts_config.yaml',\n",
    "                  './models/KR-ELECTRA-discriminator.yaml',\n",
    "                  './models/electra-kor-base.yaml']\n",
    "\n",
    "# 전부 때려 박기\n",
    "config_paths_2 = ['./models/roberta-base.yaml',\n",
    "                  './models/roberta_large.yaml',\n",
    "                  './models/kf_deberta_cross_sts_config.yaml',\n",
    "                  './models/KR-ELECTRA-discriminator.yaml',\n",
    "                  './models/electra-kor-base.yaml']\n",
    "\n",
    "# 다양한 모델, valid pearson 높았던 것 기준\n",
    "config_paths_3 = ['./models/kf_deberta_cross_sts_config.yaml',\n",
    "                  './models/electra-kor-base.yaml',\n",
    "                  './models/roberta_large.yaml']\n",
    "\n",
    "# 2와 반대의 조합\n",
    "config_paths_4 = ['./models/kf_deberta_cross_sts_config.yaml',\n",
    "                  './models/KR-ELECTRA-discriminator.yaml',\n",
    "                  './models/roberta-base.yaml']\n",
    "\n",
    "config_paths_5 = ['./models/roberta_large.yaml',\n",
    "                  './models/kf_deberta_cross_sts_config.yaml',\n",
    "                  './models/ko_electra_config.yaml',\n",
    "                  './models/electra-kor-base.yaml']\n",
    "\n",
    "config_paths_6 = ['./models/ko_electra_config.yaml',\n",
    "                  './models/kf_deberta_cross_sts_config.yaml',\n",
    "                  './models/electra-kor-base.yaml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== base predicting train data ==========\n",
      "+++++ Right now using \"klue/roberta-large\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 9324/9324 [00:03<00:00, 2939.11it/s]\n",
      "base prediction: 100%|██████████| 583/583 [04:39<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"deliciouscat/kf-deberta-base-cross-sts\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenization: 100%|██████████| 9324/9324 [00:03<00:00, 2965.89it/s]\n",
      "base prediction: 100%|██████████| 583/583 [02:13<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"monologg/koelectra-base-v3-discriminator\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 9324/9324 [00:03<00:00, 2638.00it/s]\n",
      "base prediction: 100%|██████████| 583/583 [01:29<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"kykim/electra-kor-base\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 9324/9324 [00:03<00:00, 2962.15it/s]\n",
      "base prediction: 100%|██████████| 292/292 [01:24<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========== base predicting valid data ==========\n",
      "+++++ Right now using \"klue/roberta-large\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 550/550 [00:00<00:00, 3080.99it/s]\n",
      "base prediction: 100%|██████████| 35/35 [00:16<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"deliciouscat/kf-deberta-base-cross-sts\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenization: 100%|██████████| 550/550 [00:00<00:00, 3060.02it/s]\n",
      "base prediction: 100%|██████████| 35/35 [00:07<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"monologg/koelectra-base-v3-discriminator\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 550/550 [00:00<00:00, 3062.90it/s]\n",
      "base prediction: 100%|██████████| 35/35 [00:05<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"kykim/electra-kor-base\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 550/550 [00:00<00:00, 3055.98it/s]\n",
      "base prediction: 100%|██████████| 18/18 [00:04<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "========== base predicting test data ==========\n",
      "+++++ Right now using \"klue/roberta-large\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 1100/1100 [00:00<00:00, 3066.26it/s]\n",
      "base prediction: 100%|██████████| 1100/1100 [00:39<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"deliciouscat/kf-deberta-base-cross-sts\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenization: 100%|██████████| 1100/1100 [00:00<00:00, 3055.97it/s]\n",
      "base prediction: 100%|██████████| 1100/1100 [00:30<00:00, 35.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"monologg/koelectra-base-v3-discriminator\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 1100/1100 [00:00<00:00, 3019.09it/s]\n",
      "base prediction: 100%|██████████| 1100/1100 [00:16<00:00, 67.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ Right now using \"kykim/electra-kor-base\" +++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenization: 100%|██████████| 1100/1100 [00:00<00:00, 3006.57it/s]\n",
      "base prediction: 100%|██████████| 1100/1100 [00:16<00:00, 67.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(9324, 4) (9324, 1)\n",
      "(550, 4) (550, 1)\n",
      "(1100, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ensemble import Ensemble\n",
    "\n",
    "ensemble = Ensemble(config_paths=config_paths_5,\n",
    "                    train_path='../../train_preprocess_v1.csv',\n",
    "                    valid_path='../../dev_preprocess_v1.csv',\n",
    "                    test_path='../../test_preprocess_v1.csv')\n",
    "\n",
    "print(ensemble.X_train_base.shape, ensemble.y_train.shape)\n",
    "print(ensemble.X_valid_base.shape, ensemble.y_valid.shape)\n",
    "print(ensemble.X_test_base.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== linear stacking result ==========\n",
      "    train pearson sim: [0.99909038]\n",
      "    valid pearson sim: [0.93723378]\n",
      "=========================================\n",
      "\n",
      "\n",
      "========== xgboost stacking result ==========\n",
      "    train pearson sim: [0.99867074]\n",
      "    valid pearson sim: [0.92889521]\n",
      "=========================================\n",
      "\n",
      "\n",
      "========== soft voting result ==========\n",
      "    train pearson sim: [0.99874349]\n",
      "    valid pearson sim: [0.93958898]\n",
      "=========================================\n",
      "\n",
      "\n",
      "========== linear stacking result ==========\n",
      "    train pearson sim: [0.99908937]\n",
      "    valid pearson sim: [0.93706956]\n",
      "=========================================\n",
      "\n",
      "\n",
      "========== xgboost stacking result ==========\n",
      "    train pearson sim: [0.99842974]\n",
      "    valid pearson sim: [0.92431907]\n",
      "=========================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.99842974]), array([0.92431907]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.stacking(clf=\"linear\")\n",
    "ensemble.stacking(clf=\"xgboost\")\n",
    "ensemble.soft_voting()\n",
    "ensemble.kfold_stacking(\"linear\", 3)\n",
    "ensemble.kfold_stacking(\"xgboost\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ensemble_output.csv\n"
     ]
    }
   ],
   "source": [
    "# ensemble.stacking(\"linear\")\n",
    "ensemble.inference(is_voting=True, submission_path='../../sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Using deliciouscat/kf-deberta-base-cross-sts\n",
      "Now Using snunlp/KR-ELECTRA-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Using deliciouscat/kf-deberta-base-cross-sts\n",
      "Now Using snunlp/KR-ELECTRA-discriminator\n",
      "base predictions: [[ 0.1 -0. ]], voting prediction: [0.], stacking prediction: [1.7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence_1 = \"손 틈 새로 비치는\"\n",
    "sentence_2 = \"아이유 참 좋다\"\n",
    "voting_base, voting_ensemble = ensemble.simulate(sentence_1, sentence_2, True)\n",
    "_, stacking_ensemble = ensemble.simulate(sentence_1, sentence_2, False)\n",
    "\n",
    "print(f\"base predictions: {np.round(voting_base, 1)}, voting prediction: {voting_ensemble}, stacking prediction: {stacking_ensemble}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
